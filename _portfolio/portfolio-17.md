---
title: "New loss implementation in Pytorch : Soft-DTW "
collection: talks
type: "Tutorial"
permalink: /portfolio/portfolio-17
---

<span style="color:rgba(65, 70, 75, 1)"> **Keys words** </span> \
*Time Series, DTW, Pytorch* \
<span style="color:rgba(65, 70, 75, 1)">**Objective**</span> \
*We re-implemented a differentiable version of Dynamic Time Warping (DTW) named [Soft DTW](https://arxiv.org/abs/1703.01541), which allow robust comaprison to shift, length etc. and  thereby expanding the utility of DTW in machine learning. Our work contributes to the current discourse on open-source because we have proposed an optimised losses compatible with PyTorch GPU with our own backward.* \
<img src='/images/ts/DTW.png' width='300' height='200'><img src='/images/ts/barycentre_1.png' width='300' height='200'> \
<span style="color:rgba(65, 70, 75, 1)"> **Links** </span> \
[<img src="/images/GitHub.png" alt="GitHub" width="37.5" height="12.5" />](https://github.com/b-ptiste/dtw-soft) [<img src="/images/report_icone.png" alt="Report" width="37.5" height="12.5" />](https://drive.google.com/file/d/1DLoEmERS7CLC-pVz2tVf6g5yopMTYnEZ/view?usp=drive_link) [<img src="/images/class_icone.png" alt="Report" width="37.5" height="12.5" />](http://www.laurentoudre.fr/ast.html)